#!/usr/bin/env python
# coding: utf-8

# 

# 1. ä»‹ç»

# <a id = "2" ></a>
# #### <b>What is Customer Churn?</b>
# <span style="font-size:16px;">  Customer churn is defined as when customers or subscribers discontinue doing business with a firm or service. </span>
# 
# <span style="font-size:16px;"> Customers in the telecom industry can choose from a variety of service providers and actively switch from one to the next. The telecommunications business has an annual churn rate of 15-25 percent in this highly competitive market.</span>
# 
# <span style="font-size:16px;"> Individualized customer retention is tough because most firms have a large number of customers and can't afford to devote much time to each of them. The costs would be too great, outweighing the additional revenue. However, if a corporation could forecast which customers are likely to leave ahead of time, it could focus customer retention efforts only on these "high risk" clients. The ultimate goal is to expand its coverage area and retrieve more
# customers loyalty. The core to succeed in this market lies in the customer itself. 
# </span>
# 
# <span style="font-size:16px;"> Customer churn is a critical metric because it is much less expensive to retain existing customers than it is to acquire new customers.</span>
# 
# <a id="churn"></a>
# <a id = "3" ></a>
# 
# <span style="font-size:16px;"><b>To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.</b></span> 
# 
# <span style="font-size:16px;"> To detect early signs of potential churn, one must first develop a holistic view of the customers and their interactions across numerous channels, including store/branch visits, product purchase histories, customer service calls, Web-based transactions, and social media interactions, to mention a few. </span> 
# 
# <span style="font-size:16px;">As a result, by addressing churn, these businesses may not only preserve their market position, but also grow and thrive. More customers they have in their network, the lower the cost of initiation and the larger the profit. As a result, the company's key focus for success is reducing client attrition and implementing effective retention strategy. </span> 
# <a id="reduce"></a>
# 
# <a id = "4" ></a>
# #### <b> Objectives</b>
# I will explore the data and try to answer some questions like:
# * What's the % of Churn Customers and customers that keep in with the active services?
# * Is there any patterns in Churn Customers based on the gender?
# * Is there any patterns/preference in Churn Customers based on the type of service provided?
# * What's the most profitable service types?
# * Which features and services are most profitable?
# * Many more questions that will arise during the analysis
# <a id="objective"></a>

# ___

# <a id = "5" ></a>
# # <span style="font-family:serif; font-size:28px;"> 2. Loading libraries and data</span>
# <a id="loading"></a>

# In[3]:


import pandas as pd
import numpy as np
#missingnoæ¨¡å—ï¼ˆç¼ºå¤±å€¼å¯è§†åŒ–ï¼‰
import missingno as msno  
# Matplotlib æ˜¯ Python çš„ç»˜å›¾åº“ï¼Œå¯ä»¥ç”¨æ¥ç»˜åˆ¶å„ç§é™æ€ï¼ŒåŠ¨æ€ï¼Œäº¤äº’å¼çš„å›¾è¡¨ï¼Œæä¾›å¤šæ ·åŒ–çš„è¾“å‡ºæ ¼å¼ã€‚
# é€šå¸¸ä¸ NumPy å’Œ SciPyï¼ˆScientific Pythonï¼‰ä¸€èµ·ä½¿ç”¨ï¼Œ è¿™ç§ç»„åˆå¹¿æ³›ç”¨äºæ›¿ä»£ MatLab
# SciPy åŒ…å«çš„æ¨¡å—æœ‰æœ€ä¼˜åŒ–ã€çº¿æ€§ä»£æ•°ã€ç§¯åˆ†ã€æ’å€¼ã€ç‰¹æ®Šå‡½æ•°ã€å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ã€ä¿¡å·å¤„ç†å’Œå›¾åƒå¤„ç†ã€å¸¸å¾®åˆ†æ–¹ç¨‹æ±‚è§£å’Œå…¶ä»–ç§‘å­¦ä¸å·¥ç¨‹ä¸­å¸¸ç”¨çš„è®¡ç®—
import matplotlib.pyplot as plt
# Seaborn integrates closely with Pandas data structures, 
# making it easy to work with dataframes and arrays
# å»ºç«‹åœ¨ Matplotlib åŸºç¡€ä¹‹ä¸Šçš„ Python æ•°æ®å¯è§†åŒ–åº“ï¼Œä¸“æ³¨äºç»˜åˆ¶å„ç§ç»Ÿè®¡å›¾å½¢
import seaborn as sns
# Plotly Express æ˜¯ä¸€ä¸ªé«˜çº§çš„Pythonæ•°æ®å¯è§†åŒ–åº“ï¼Œå®ƒæ˜¯Plotly.pyçš„å°è£…ï¼Œæä¾›äº†ä¸€ä¸ªç®€æ´ä¸”ä¸€è‡´çš„APIæ¥åˆ›å»ºå¤æ‚çš„å›¾è¡¨ã€‚
import plotly.express as px
# Plotly æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ Python æ•°æ®å¯è§†åŒ–åº“ï¼Œæä¾›äº†ä¸°å¯Œçš„å›¾è¡¨ç±»å‹å’Œçµæ´»çš„å®šåˆ¶é€‰é¡¹ã€‚Plotly çš„å›¾å½¢å¯¹è±¡ï¼ˆGraph Objectsï¼‰æ¨¡å—ï¼ˆé€šå¸¸å¯¼å…¥ä¸º goï¼‰åŒ…å«äº†ä¸€ç³»åˆ—è‡ªåŠ¨ç”Ÿæˆçš„ Python ç±»ï¼Œè¿™äº›ç±»è¡¨ç¤ºå›¾å½¢çš„å„ä¸ªéƒ¨åˆ†
import plotly.graph_objects as go
# ç»˜åˆ¶å­å›¾
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')


# In[4]:


# StandardScaleræ˜¯sklearnä¸­çš„ä¸€ä¸ªå½’ä¸€åŒ–å·¥å…·ï¼Œå¯ä»¥å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è¿›è¡Œå»å‡å€¼å’Œæ–¹å·®æ ‡å‡†åŒ–ï¼Œä½¿æ•°æ®ç¬¦åˆæ ‡å‡†æ­£æ€åˆ†å¸ƒ
from sklearn.preprocessing import StandardScaler
# LabelEncoder æ˜¯ sklearn ä¸­ç”¨äºç±»åˆ«æ ‡ç­¾ç¼–ç çš„é‡è¦å·¥å…·ï¼Œèƒ½å¤Ÿå°†ç¦»æ•£çš„ç±»åˆ«å‹æ ‡ç­¾è½¬æ¢ä¸ºæ¨¡å‹å¯è¯†åˆ«çš„æ•°å€¼æ ¼å¼
from sklearn.preprocessing import LabelEncoder
# å†³ç­–æ ‘åˆ†ç±»å™¨
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
# é«˜æ–¯æœ´ç´ è´å¶æ–¯ å…ˆéªŒæ¦‚ç‡priors
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
# MLPClassifier æ˜¯ä¸€ä¸ªç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œå®ƒæ˜¯å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰çš„ä¸€ç§ï¼Œä¹Ÿç§°ä¸ºäººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰ã€‚MLPClassifierå¯ä»¥å¤„ç†åŒ…æ‹¬åˆ†ç±»é—®é¢˜åœ¨å†…çš„å¤šç§æœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚å®ƒé€šè¿‡å­¦ä¹ è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„æ˜ å°„å…³ç³»æ¥è¿›è¡Œé¢„æµ‹ã€‚
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
# æç«¯éšæœºæ ‘
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# XGBClassifieræ˜¯åŸºäºæ¢¯åº¦æå‡çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå®ƒå¯ä»¥å¤„ç†ç¼ºå¤±å€¼ã€æ”¯æŒå¹¶è¡Œè®¡ç®—ï¼Œå¹¶ä¸”å…·æœ‰å†…ç½®çš„äº¤å‰éªŒè¯åŠŸèƒ½
from xgboost import XGBClassifier
# CatBoostæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„æœºå™¨å­¦ä¹ åº“ï¼Œå®ƒåŸºäºå¯¹ç§°å†³ç­–æ ‘ï¼ˆoblivious treesï¼‰ä½œä¸ºåŸºå­¦ä¹ å™¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ç±»åˆ«å‹ç‰¹å¾ã€‚CatBoostClassifieræ˜¯CatBooståº“ä¸­ç”¨äºåˆ†ç±»é—®é¢˜çš„ç»„ä»¶ï¼Œå®ƒæä¾›äº†ä¸°å¯Œçš„å‚æ•°ç”¨äºæ¨¡å‹çš„è®­ç»ƒå’Œä¼˜åŒ–
from catboost import CatBoostClassifier
from sklearn import metrics
from sklearn.metrics import roc_curve
from sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report


# In[5]:


#loading data
# df = pd.read_csv('../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')
df=pd.read_csv(r'E:\Code\Python\customer-churn-prediction\data\raw\WA_Fn-UseC_-Telco-Customer-Churn.csv') 


# ___

# <a id = "6" ></a>
# # <span style="font-family:serif; font-size:28px;"> 3. Undertanding the data</span>
# <a id = "Undertanding the data" ></a>

# Each row represents a customer, each column contains customerâ€™s attributes described on the column Metadata.

# In[6]:


df.head()


# **The data set includes information about:**
# * **Customers who left within the last month** â€“ the column is called Churn
# 
# * **Services that each customer has signed up for** â€“ phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies
# 
# * **Customer account information** - how long theyâ€™ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges
# 
# * **Demographic info about customers** â€“ gender, age range, and if they have partners and dependents
# 
# 
# * ***ç¿»è¯‘*** - å®¢æˆ·ID,æ€§åˆ«,è€å¹´å…¬æ°‘,ä¼´ä¾£,å®¶å±,å…¥ç½‘æ—¶é•¿,ç”µè¯æœåŠ¡,å¤šçº¿è·¯,äº’è”ç½‘æœåŠ¡,åœ¨çº¿å®‰å…¨,åœ¨çº¿å¤‡ä»½,è®¾å¤‡ä¿æŠ¤,æŠ€æœ¯æ”¯æŒ,ç”µè§†æµåª’ä½“,ç”µå½±æµåª’ä½“,åˆåŒ,æ— çº¸åŒ–è´¦å•,ä»˜æ¬¾æ–¹å¼,æ¯æœˆè´¹ç”¨,æ€»è´¹ç”¨,æµå¤±
# 

# In[7]:


df.shape


# In[8]:


df.info()


# In[9]:


df.columns.values


# In[10]:


df.dtypes


# 
# * The target the we will use to guide the exploration is **Churn**

# ***

# <a id = "7" ></a>
# # <span style="font-family:serif; font-size:28px;"> 4. Visualize missing values </span>
# <a id = "missingvalue" ></a>

# In[11]:


# Visualize missing values as a matrix
msno.matrix(df);


# > Using this matrix we can very quickly find the pattern of missingness in the dataset. 
# * From the above visualisation we can observe that it has no peculiar pattern that stands out. In fact there is no missing data.

# ***

# <a id = "8" ></a>
# # <span style="font-family:serif; font-size:28px;"> 5. Data Manipulation </span>
# <a id = "8" ></a>

# In[12]:


df = df.drop(['customerID'], axis = 1)
df.head()


# * On deep analysis, we can find some indirect missingness in our data (which can be in form of blankspaces). Let's see that!

# In[13]:


# pandas.to_numeric() æ˜¯ pandas é¡¶çº§å‡½æ•°ï¼Œè¯­æ³•æ˜¯
# pandas.to_numeric(arg, errors='raise', downcast=None)
# errors : å¯ä¼ å…¥ {'ignore', 'raise', 'coerce'}, é»˜è®¤ 'raise'ï¼Œå¦‚æœæ— æ³•è§£ææ•°æ®çš„å¤„ç†æ–¹æ¡ˆã€‚
# 'raise', å¦‚æœæ— æ³•è§£æå°†å¼•å‘å¼‚å¸¸
# 'coerce', å¦‚æœæ— æ³•è§£æå°†è®¾ç½®ä¸º NaN
# 'ignore', ç„¶åæ— æ•ˆè§£æå°†è¿”å›è¾“å…¥
# downcast : str, é»˜è®¤ Noneï¼Œé™çº§å¤„ç†ã€å‘ä¸‹è½¬æ¢ã€‚å¯ä¼ å…¥å€¼æœ‰ 'integer', 'signed', 'unsigned', æˆ–è€… 'float'ã€‚
df['TotalCharges'] = pd.to_numeric(df.TotalCharges, errors='coerce')
df.isnull().sum()


# * Here we see that the TotalCharges has 11 missing values. Let's check this data.

# In[14]:


df[np.isnan(df['TotalCharges'])]


# * It can also be noted that the Tenure column is 0 for these entries even though the MonthlyCharges column is not empty.
# 
# Let's see if there are any other 0 values in the tenure column.

# In[15]:


df[df['tenure'] == 0].index


# * There are no additional missing values in the Tenure column. 
# 
# Let's delete the rows with missing values in Tenure columns since there are only 11 rows and deleting them will not affect the data.

# In[16]:


df.drop(labels=df[df['tenure'] == 0].index, axis=0, inplace=True)
df[df['tenure'] == 0].index
df1=df.copy()
df1.info()
df1['TotalCharges'] = pd.to_numeric(df1.TotalCharges, errors='coerce')
df1.isnull().sum()


# > To solve the problem of missing values in TotalCharges column, I decided to fill it with the mean of TotalCharges values.

# In[17]:


# åŸæœ¬çš„dfå·²ç»åˆ é™¤ç¼ºå¤±çš„11è¡Œï¼Œå¹¶è¢«æ›¿æ¢äº†ï¼Œä¸ºå•¥è¿˜æ’å€¼ï¼Ÿ
df.fillna(df["TotalCharges"].mean())


# In[18]:


df.isnull().sum()


# In[19]:


df["SeniorCitizen"]= df["SeniorCitizen"].map({0: "No", 1: "Yes"})
df.head()


# In[20]:


df["InternetService"].describe(include=['object', 'bool'])


# In[21]:


numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']
df[numerical_cols].describe()


# ___

# <a id = "9" ></a>
# # <span style="font-family:serif; font-size:28px;"> 6. Data Visualization </span>
# <a id = "datavisualization" ></a>

# In[22]:


g_labels = ['Male', 'Female']
c_labels = ['No', 'Yes']
# Create subplots: use 'domain' type for Pie subplot
fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])
fig.add_trace(go.Pie(labels=g_labels, values=df['gender'].value_counts(), name="Gender"),
              1, 1)
fig.add_trace(go.Pie(labels=c_labels, values=df['Churn'].value_counts(), name="Churn"),
              1, 2)

# Use `hole` to create a donut-like pie chart
fig.update_traces(hole=.4, hoverinfo="label+percent+name", textfont_size=16)

fig.update_layout(
    title_text="Gender and Churn Distributions",
    # Add annotations in the center of the donut pies.
    annotations=[dict(text='Gender', x=0.16, y=0.5, font_size=20, showarrow=False),
                 dict(text='Churn', x=0.84, y=0.5, font_size=20, showarrow=False)])
fig.show()


# * 26.6 % of customers switched to another firm.
# * Customers are 49.5 % female and 50.5 % male.

# In[23]:


df["Churn"][df["Churn"]=="No"].groupby(by=df["gender"]).count()
# df["Churn"][df["Churn"]=="No"].groupby(by=df["gender"]).count()


# In[24]:


df["Churn"][df["Churn"]=="Yes"].groupby(by=df["gender"]).count()


# In[25]:


plt.figure(figsize=(6, 6))
labels =["Churn: Yes","Churn:No"]
values = [1869,5163]
labels_gender = ["F","M","F","M"]
sizes_gender = [939,930 , 2544,2619]
colors = ['#ff6666', '#66b3ff']
colors_gender = ['#c2c2f0','#ffb3e6', '#c2c2f0','#ffb3e6']
explode = (0.3,0.3) 
explode_gender = (0.1,0.1,0.1,0.1)
textprops = {"fontsize":15}
#Plot
plt.pie(values, labels=labels,autopct='%1.1f%%',pctdistance=1.08, labeldistance=0.8,colors=colors, startangle=90,frame=True, explode=explode,radius=10, textprops =textprops, counterclock = True, )
plt.pie(sizes_gender,labels=labels_gender,colors=colors_gender,startangle=90, explode=explode_gender,radius=7, textprops =textprops, counterclock = True, )
#Draw circle
centre_circle = plt.Circle((0,0),5,color='black', fc='white',linewidth=0)
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

plt.title('Churn Distribution w.r.t Gender: Male(M), Female(F)', fontsize=15, y=1.1)

# show plot 
 
plt.axis('equal')
plt.tight_layout()
plt.show()


# * There is negligible difference in customer percentage/ count who chnaged the service provider. Both genders behaved in similar fashion when it comes to migrating to another service provider/firm.

# In[26]:


fig = px.histogram(df, x="Churn", color="Contract", barmode="group", title="<b>Customer contract distribution<b>")
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()


# * About 75% of customer with Month-to-Month Contract opted to move out as compared to 13% of customrs with One Year Contract and 3% with Two Year Contract

# In[27]:


labels = df['PaymentMethod'].unique()
values = df['PaymentMethod'].value_counts()
print(labels)
print(values)

fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])
fig.update_layout(title_text="<b>Payment Method Distribution</b>")
fig.show()


# In[28]:


fig = px.histogram(df, x="Churn", color="PaymentMethod", title="<b>Customer Payment Method distribution w.r.t. Churn</b>")
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()


# * Major customers who moved out were having Electronic Check as Payment Method.
# * Customers who opted for Credit-Card automatic transfer or Bank Automatic Transfer and Mailed Check as Payment Method were less likely to move out.  

# In[29]:


df["InternetService"].unique()


# In[30]:


df[df["gender"]=="Male"][["InternetService", "Churn"]].value_counts()


# In[31]:


df[df["gender"]=="Female"][["InternetService", "Churn"]].value_counts()


# In[32]:


fig = go.Figure()

fig.add_trace(go.Bar(
  x = [['Churn:No', 'Churn:No', 'Churn:Yes', 'Churn:Yes'],
       ["Female", "Male", "Female", "Male"]],
  y = [965, 992, 219, 240],
  name = 'DSL',
))

fig.add_trace(go.Bar(
  x = [['Churn:No', 'Churn:No', 'Churn:Yes', 'Churn:Yes'],
       ["Female", "Male", "Female", "Male"]],
  y = [889, 910, 664, 633],
  name = 'Fiber optic',
))

fig.add_trace(go.Bar(
  x = [['Churn:No', 'Churn:No', 'Churn:Yes', 'Churn:Yes'],
       ["Female", "Male", "Female", "Male"]],
  y = [690, 717, 56, 57],
  name = 'No Internet',
))

fig.update_layout(title_text="<b>Churn Distribution w.r.t. Internet Service and Gender</b>")

fig.show()


# * A lot of customers choose the Fiber optic service and it's also evident that the customers who use Fiber optic have high churn rate, this might suggest a dissatisfaction with this type of internet service.
# * Customers having DSL service are majority in number and have less churn rate compared to Fibre optic service.

# In[33]:


color_map = {"Yes": "#FF97FF", "No": "#AB63FA"}
fig = px.histogram(df, x="Churn", color="Dependents", barmode="group", title="<b>Dependents distribution</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()


# * Customers without dependents are more likely to churn

# In[34]:


color_map = {"Yes": '#FFA15A', "No": '#00CC96'}
fig = px.histogram(df, x="Churn", color="Partner", barmode="group", title="<b>Chrun distribution w.r.t. Partners</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()


# * Customers that doesn't have partners are more likely to churn

# In[35]:


color_map = {"Yes": '#00CC96', "No": '#B6E880'}
fig = px.histogram(df, x="Churn", color="SeniorCitizen", title="<b>Chrun distribution w.r.t. Senior Citizen</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()


# * It can be observed that the fraction of senior citizen is very less.
# * Most of the senior citizens churn.

# In[36]:


color_map = {"Yes": "#FF97FF", "No": "#AB63FA"}
fig = px.histogram(df, x="Churn", color="OnlineSecurity", barmode="group", title="<b>Churn w.r.t Online Security</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()


# * Most customers churn in the absence of online security, 

# In[37]:


color_map = {"Yes": '#FFA15A', "No": '#00CC96'}
fig = px.histogram(df, x="Churn", color="PaperlessBilling",  title="<b>Chrun distribution w.r.t. Paperless Billing</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()


# * Customers with Paperless Billing are most likely to churn.

# In[38]:


fig = px.histogram(df, x="Churn", color="TechSupport",barmode="group",  title="<b>Chrun distribution w.r.t. TechSupport</b>")
fig.update_layout(width=700, height=500, bargap=0.1)
try:
    fig.show()
except Exception as e:
    print(f"Error displaying figure: {e}")   


# * Customers with no TechSupport are most likely to migrate to another service provider.

# In[39]:


color_map = {"Yes": '#00CC96', "No": '#B6E880'}
fig = px.histogram(df, x="Churn", color="PhoneService", title="<b>Chrun distribution w.r.t. Phone Service</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()


# * Very small fraction of customers don't have a phone service and out of that, 1/3rd Customers are more likely to churn. åªæœ‰æå°‘æ•°å®¢æˆ·æ²¡æœ‰ç”µè¯æœåŠ¡ï¼Œå…¶ä¸­ 1/3 çš„å®¢æˆ·æ›´æœ‰å¯èƒ½æµå¤±ã€‚

# In[40]:


# æ ¸å¯†åº¦ä¼°è®¡ï¼ˆKDEï¼‰æ˜¯ä¸€ç§ç”¨äºä¼°è®¡éšæœºå˜é‡æ¦‚ç‡å¯†åº¦å‡½æ•°çš„éå‚æ•°æ–¹æ³•ã€‚åœ¨seabornåº“ä¸­ï¼Œkdeplotå‡½æ•°æä¾›äº†ä¸€ç§æ–¹ä¾¿çš„æ–¹å¼æ¥å¯è§†åŒ–å•å˜é‡æˆ–åŒå˜é‡çš„åˆ†å¸ƒã€‚
# è¿™ä¸ªå‡½æ•°ä¼šç”Ÿæˆä¸€ä¸ªè¿ç»­çš„æ¦‚ç‡å¯†åº¦æ›²çº¿ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£æ•°æ®çš„åˆ†å¸ƒç‰¹å¾ã€‚
# æ•°æ®åˆ†å¸ƒï¼ˆData Distributionï¼‰ æŒ‡çš„æ˜¯ä¸€ç»„æ•°æ®ä¸­å„ä¸ªå€¼çš„å‡ºç°é¢‘ç‡æˆ–æ¦‚ç‡ï¼Œæè¿°äº†æ•°æ®åœ¨æ•°è½´ä¸Šçš„åˆ†å¸ƒå½¢æ€ã€é›†ä¸­è¶‹åŠ¿ã€ç¦»æ•£ç¨‹åº¦ç­‰ç‰¹å¾ã€‚
sns.set_context("paper",font_scale=1.1)
ax = sns.kdeplot(df.MonthlyCharges[(df["Churn"] == 'No') ],
                color="Red", shade = True);
ax = sns.kdeplot(df.MonthlyCharges[(df["Churn"] == 'Yes') ],
                ax =ax, color="Blue", shade= True);
ax.legend(["Not Churn","Churn"],loc='upper right');
ax.set_ylabel('Density');
ax.set_xlabel('Monthly Charges');
ax.set_title('Distribution of monthly charges by churn');


# * Customers with higher Monthly Charges are also more likely to churn

# In[41]:


ax = sns.kdeplot(df.TotalCharges[(df["Churn"] == 'No') ],
                color="Gold", shade = True);
ax = sns.kdeplot(df.TotalCharges[(df["Churn"] == 'Yes') ],
                ax =ax, color="Green", shade= True);
ax.legend(["Not Chu0rn","Churn"],loc='upper right');
ax.set_ylabel('Density');
ax.set_xlabel('Total Charges');
ax.set_title('Distribution of total charges by churn');


# In[42]:


fig = px.box(df, x='Churn', y = 'tenure')

# Update yaxis properties
fig.update_yaxes(title_text='Tenure (Months)', row=1, col=1)
# Update xaxis properties
fig.update_xaxes(title_text='Churn', row=1, col=1)

# Update size and title
fig.update_layout(autosize=True, width=750, height=600,
    title_font=dict(size=25, family='Courier'),
    title='<b>Tenure vs Churn</b>',
)

fig.show()


# * New customers are more likely to churn

# In[43]:


plt.figure(figsize=(25, 10))

# df.apply(function,axis) éå†ä¸€è¡Œaxis=1æˆ–ä¸€åˆ—axis=0(é»˜è®¤)
# lambda:å‡½æ•°å¼ç¼–ç¨‹
# # factorize()  Example array åˆ†ç±»å˜é‡è½¬æ¢ä¸ºæ•´æ•°ç¼–ç 
# arr = np.array(['b', 'b', 'a', 'c', 'b'], dtype="O")
# # Factorize the array
# codes, uniques = pd.factorize(arr)
# print("Codes:", codes) # Output: [0, 0, 1, 2, 0]
# print("Uniques:", uniques) # Output: ['b', 'a', 'c']
# corré€šå¸¸æ˜¯ä¸€ä¸ªç›¸å…³ç³»æ•°çŸ©é˜µï¼ˆå¦‚é€šè¿‡pandas.DataFrame.corr()è®¡ç®—å¾—åˆ°ï¼‰ï¼Œå½¢çŠ¶ä¸º(n, n)
corr = df.apply(lambda x: pd.factorize(x)[0]).corr()   

# np.ones_like()åˆ›å»ºä¸€ä¸ªä¸ç»™å®šæ•°ç»„å½¢çŠ¶å’Œç±»å‹ç›¸åŒçš„æ–°æ•°ç»„ï¼Œä½†æ–°æ•°ç»„çš„æ‰€æœ‰å…ƒç´ éƒ½æ˜¯1
# np.triu æ˜¯NumPyåº“ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºæå–çŸ©é˜µçš„ä¸Šä¸‰è§’?éƒ¨åˆ†
mask = np.triu(np.ones_like(corr, dtype=bool))
# ä½¿ç”¨seaborn.heatmap()ä¼ å…¥æ©ç ï¼Œé¿å…é‡å¤æ˜¾ç¤ºå¯¹ç§°ä¿¡æ¯
ax = sns.heatmap(
    corr,                   # ç›¸å…³ç³»æ•°çŸ©é˜µï¼ˆé€šå¸¸æ˜¯DataFrameæˆ–numpyæ•°ç»„ï¼‰
    mask=mask,              # æ©ç ï¼šéšè—ä¸‹ä¸‰è§’éƒ¨åˆ†ï¼ˆåŒ…æ‹¬å¯¹è§’çº¿ï¼‰
    xticklabels=corr.columns,  # Xè½´æ ‡ç­¾ä½¿ç”¨DataFrameçš„åˆ—å
    yticklabels=corr.columns,  # Yè½´æ ‡ç­¾ä½¿ç”¨DataFrameçš„åˆ—å
    annot=True,             # åœ¨æ¯ä¸ªå•å…ƒæ ¼ä¸­æ˜¾ç¤ºç›¸å…³ç³»æ•°å€¼
    linewidths=.2,          # å•å…ƒæ ¼ä¹‹é—´çš„åˆ†éš”çº¿å®½åº¦
    cmap='coolwarm',        # é¢œè‰²æ˜ å°„ï¼šä»è“è‰²ï¼ˆ-1ï¼‰åˆ°ç™½è‰²ï¼ˆ0ï¼‰åˆ°çº¢è‰²ï¼ˆ+1ï¼‰
    vmin=-1, vmax=1         # é¢œè‰²æ˜ å°„çš„å–å€¼èŒƒå›´ï¼šç›¸å…³ç³»æ•°çš„ç†è®ºèŒƒå›´æ˜¯[-1, 1]
)


# ___

# <a id = "10" ></a>
# # <span style="font-family:serif; font-size:28px;"> 7. Data Preprocessing</span>
# <a id = "datapreprocessing" ></a>

# <a id = "1111" ></a>
# #### **Splitting the data into train and test sets**
# <a id = "Split" ></a>

# In[44]:


def object_to_int(dataframe_series):
    if dataframe_series.dtype=='object':
        dataframe_series = LabelEncoder().fit_transform(dataframe_series) # å°†å­—ç¬¦ä¸²ç¼–ç ä¸ºæ•´æ•°
    return dataframe_series


# In[45]:


# df.apply(...)ï¼šå¯¹ DataFrame çš„æ¯ä¸€åˆ—ï¼ˆé»˜è®¤ axis=0ï¼‰åº”ç”¨è‡ªå®šä¹‰å‡½æ•°ã€‚
df = df.apply(lambda x: object_to_int(x))  #åŒ¿åå‡½æ•°ï¼Œå…¶ä¸­ x ä»£è¡¨ DataFrame çš„æ¯ä¸€åˆ—ï¼ˆå³ä¸€ä¸ª pandas Seriesï¼‰
df.head()


# In[46]:


plt.figure(figsize=(14,7))
# print(df.corr())
df.corr()
# è®¡ç®— DataFrame ä¸­æ‰€æœ‰æ•°å€¼åˆ—ä¹‹é—´çš„çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆé»˜è®¤æ–¹æ³•ï¼‰ï¼Œè¿”å›ä¸€ä¸ªç›¸å…³ç³»æ•°çŸ©é˜µ
# ['Churn']ï¼šä»ç›¸å…³ç³»æ•°çŸ©é˜µä¸­æå–ä¸Churnåˆ—ç›¸å…³çš„æ‰€æœ‰ç³»æ•°ï¼Œå¾—åˆ°ä¸€ä¸ª Series
df.corr()['Churn'].sort_values(ascending = False)


# In[47]:


X = df.drop(columns = ['Churn'])
y = df['Churn'].values
print(df["Churn"])
print(df['Churn'].values)


# In[48]:


# stratify=y
# åˆ†å±‚æŠ½æ ·ï¼šç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ç›®æ ‡å˜é‡ y çš„ç±»åˆ«æ¯”ä¾‹ä¸åŸå§‹æ•°æ®ä¸€è‡´ã€‚
# é€‚ç”¨åœºæ™¯ï¼šå¤„ç†ä¸å¹³è¡¡æ•°æ®é›†ï¼ˆå¦‚æ­£ç±»æ ·æœ¬å  10%ï¼Œè´Ÿç±»å  90%ï¼‰ï¼Œé˜²æ­¢è®­ç»ƒ / æµ‹è¯•é›†åˆ†å¸ƒåå·®å¯¼è‡´æ¨¡å‹å¤±æ•ˆã€‚
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30, random_state = 40, stratify=y)

# ç¡®ä¿åœ¨åˆ’åˆ†åå†è¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼ˆå¦‚æ ‡å‡†åŒ–ã€ç‰¹å¾é€‰æ‹©ï¼‰ã€‚è‹¥åœ¨åˆ’åˆ†å‰å¤„ç†ï¼Œæµ‹è¯•é›†å¯èƒ½ â€œå·çœ‹â€ åˆ°è®­ç»ƒé›†çš„ç»Ÿè®¡ä¿¡æ¯ã€‚


# In[50]:


def distplot(feature, frame, color='r'):
    plt.figure(figsize=(8,3))
    plt.title("Distribution for {}".format(feature))
    ax = sns.distplot(frame[feature], color= color)


# 

# In[51]:


num_cols = ["tenure", 'MonthlyCharges', 'TotalCharges']
for feat in num_cols: distplot(feat, df)


# Since the numerical features are distributed over different value ranges, I will use standard scalar to scale them down to the same range.

# <a id = "111" ></a>
# #### **Standardizing numeric attributes**
# <a id = "Standardizing" ></a>

# In[52]:


df_std = pd.DataFrame(StandardScaler().fit_transform(df[num_cols].astype('float64')),
                       columns=num_cols)
for feat in numerical_cols: distplot(feat, df_std, color='c')


# 

# In[53]:


# Divide the columns into 3 categories, one ofor standardisation, one for label encoding and one for one hot encoding
# æ‰‹åŠ¨æŒ‡å®šéœ€è¦ç‹¬çƒ­ç¼–ç çš„åˆ—ï¼ˆé€šå¸¸æ˜¯æ— åºåˆ†ç±»å˜é‡ï¼‰:
cat_cols_ohe =['PaymentMethod', 'Contract', 'InternetService'] # those that need one-hot encoding
cat_cols_le = list(set(X_train.columns)- set(num_cols) - set(cat_cols_ohe)) #those that need label encoding
print("éœ€è¦æ ‡ç­¾ç¼–ç çš„åˆ—:",cat_cols_le)


# In[54]:


scaler= StandardScaler()

X_train[num_cols] = scaler.fit_transform(X_train[num_cols])
X_test[num_cols] = scaler.transform(X_test[num_cols])


# <a id = "11" ></a>
# # <span style="font-family:serif; font-size:28px;"> 8. Machine Learning Model Evaluations and Predictions</span>
# <a id = "modelprediction" ></a>

# ![AI-Workbench-Predict-propensity-churn-notebook.png](attachment:8fc66a4b-838f-401e-bf6b-4577d1f313ec.png)

# <a id = "101" ></a>
# #### <b> KNN</b>
# <a id = "knn" ></a>

# In[55]:


knn_model = KNeighborsClassifier(n_neighbors = 11, n_jobs=1) 
knn_model.fit(X_train,y_train)
predicted_y = knn_model.predict(X_test)
accuracy_knn = knn_model.score(X_test,y_test)
print("KNN accuracy:",accuracy_knn)


# In[56]:


print(classification_report(y_test, predicted_y))


# <a id = "102" ></a>
# #### <b>SVC</b>
# <a id = "svc" ></a>

# In[57]:


svc_model = SVC(random_state = 1)
svc_model.fit(X_train,y_train)
predict_y = svc_model.predict(X_test)
accuracy_svc = svc_model.score(X_test,y_test)
print("SVM accuracy is :",accuracy_svc)


# In[58]:


print(classification_report(y_test, predict_y))


# <a id = "103" ></a>
# #### <b> Random Forest</b>
# <a id = "rf" ></a>

# In[63]:


model_rf = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,
                                  random_state =50, max_features = "sqrt",
                                  max_leaf_nodes = 30)
model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print (metrics.accuracy_score(y_test, prediction_test))
accuracy_rf = model_rf.score(X_test,y_test)
print("RF accuracy is :",accuracy_rf)


# In[64]:


print(classification_report(y_test, prediction_test))


# In[65]:


plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, prediction_test),
                annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title(" RANDOM FOREST CONFUSION MATRIX",fontsize=14)
plt.show()


# In[66]:


y_rfpred_prob = model_rf.predict_proba(X_test)[:,1]
fpr_rf, tpr_rf, thresholds = roc_curve(y_test, y_rfpred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr_rf, tpr_rf, label='Random Forest',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest ROC Curve',fontsize=16)
plt.show();


# <a id = "104" ></a>
# #### <b>Logistic Regression</b>
# <a id = "lr" ></a>

# In[67]:


lr_model = LogisticRegression()
lr_model.fit(X_train,y_train)
accuracy_lr = lr_model.score(X_test,y_test)
print("Logistic Regression accuracy is :",accuracy_lr)


# In[68]:


lr_pred= lr_model.predict(X_test)
report = classification_report(y_test,lr_pred)
print(report)


# In[69]:


plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, lr_pred),
                annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("LOGISTIC REGRESSION CONFUSION MATRIX",fontsize=14)
plt.show()


# In[70]:


y_pred_prob = lr_model.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='Logistic Regression',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression ROC Curve',fontsize=16)
plt.show();


# <a id = "105" ></a>
# #### **Decision Tree Classifier**
# <a id = "dtc" ></a>

# In[71]:


dt_model = DecisionTreeClassifier()
dt_model.fit(X_train,y_train)
predictdt_y = dt_model.predict(X_test)
accuracy_dt = dt_model.score(X_test,y_test)
print("Decision Tree accuracy is :",accuracy_dt)


# 

# Decision tree gives very low score.

# In[72]:


print(classification_report(y_test, predictdt_y))


# <a id = "106" ></a>
# #### **AdaBoost Classifier**
# <a id = "ada" ></a>

# In[73]:


a_model = AdaBoostClassifier()
a_model.fit(X_train,y_train)
a_preds = a_model.predict(X_test)
print("AdaBoost Classifier accuracy")
metrics.accuracy_score(y_test, a_preds)


# In[74]:


print(classification_report(y_test, a_preds))


# In[75]:


plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, a_preds),
                annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("AdaBoost Classifier Confusion Matrix",fontsize=14)
plt.show()


# <a id = "107" ></a>
# #### **Gradient Boosting Classifier**
# <a id = "gb" ></a>

# In[76]:


gb = GradientBoostingClassifier()
gb.fit(X_train, y_train)
gb_pred = gb.predict(X_test)
print("Gradient Boosting Classifier", accuracy_score(y_test, gb_pred))


# In[77]:


print(classification_report(y_test, gb_pred))


# In[78]:


plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, gb_pred),
                annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("Gradient Boosting Classifier Confusion Matrix",fontsize=14)
plt.show()


# <a id = "108" ></a>
# #### **Voting Classifier**
# <a id = "vc" ></a>
# Let's now predict the final model based on the highest majority of voting and check it's score.

# In[89]:


from sklearn.ensemble import VotingClassifier
# GradientBoostingClassifierï¼šæ¢¯åº¦æå‡æ ‘ï¼Œé€‚åˆæ•æ‰æ•°æ®ä¸­çš„éçº¿æ€§å…³ç³»
# LogisticRegressionï¼šé€»è¾‘å›å½’ï¼Œæä¾›çº¿æ€§åˆ†ç±»è¾¹ç•Œå’Œæ¦‚ç‡è¾“å‡º
# AdaBoostClassifierï¼šè‡ªé€‚åº”æå‡ç®—æ³•ï¼Œé€šè¿‡ç»„åˆå¼±åˆ†ç±»å™¨æé«˜æ•´ä½“æ€§èƒ½
clf1 = GradientBoostingClassifier()
clf2 = LogisticRegression()  
clf3 = AdaBoostClassifier()
# estimators å‚æ•°æ˜¯ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å« (åç§°ï¼Œæ¨¡å‹)
# voting='soft' è¡¨ç¤ºä½¿ç”¨è½¯æŠ•ç¥¨æœºåˆ¶ï¼šåŸºäºå„æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡è¿›è¡ŒåŠ æƒå¹³å‡
# è½¯æŠ•ç¥¨è¦æ±‚æ‰€æœ‰åŸºç¡€æ¨¡å‹éƒ½èƒ½æä¾›æ¦‚ç‡é¢„æµ‹ï¼ˆå³å…·æœ‰ predict_proba æ–¹æ³•ï¼‰
eclf1 = VotingClassifier(estimators=[('gbc', clf1), ('lr', clf2), ('abc', clf3)], voting='soft')
eclf1.fit(X_train, y_train)
predictions = eclf1.predict(X_test)
print("Final Accuracy Score ")
print(accuracy_score(y_test, predictions))


# In[80]:


print(classification_report(y_test, predictions))


# In[81]:


plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, predictions),
                annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("FINAL CONFUSION MATRIX",fontsize=14)
plt.show()


# From the confusion matrix we can see that: 
# There are total 1400+149=1549 actual non-churn values and the algorithm predicts 1400 of them as non churn and 149 of them as churn.
# While there are 237+324=561 actual churn values and the algorithm predicts 237 of them as non churn values and 324 of them as churn values.
# 
# ä»æ··æ·†çŸ©é˜µä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ° å®é™…éæµå¤±å€¼å…±æœ‰ 1400+149=1549 ä¸ªï¼Œç®—æ³•é¢„æµ‹å…¶ä¸­ 1400 ä¸ªä¸ºéæµå¤±å€¼ï¼Œ149 ä¸ªä¸ºæµå¤±å€¼ã€‚è€Œå®é™…æµå¤±å€¼ä¸º 237+324=561 ä¸ªï¼Œç®—æ³•é¢„æµ‹å…¶ä¸­ 237 ä¸ªä¸ºéæµå¤±å€¼ï¼Œ324 ä¸ªä¸ºæµå¤±å€¼ã€‚

# Customer churn is definitely bad to a firm â€™s profitability. Various strategies can be implemented to eliminate customer churn. The best way to avoid customer churn is for a company to truly know its customers. This includes identifying customers who are at risk of churning and working to improve their satisfaction. Improving customer service is, of course, at the top of the priority for tackling this issue. Building customer loyalty through relevant experiences and specialized service is another strategy to reduce customer churn. Some firms survey customers who have already churned to understand their reasons for leaving in order to adopt a proactive approach to avoiding future customer churn. 

# <span style="color:crimson;font-family:serif; font-size:20px;">  Please upvote if you liked the kernel! ğŸ˜€
#     <p style="color:royalblue;font-family:serif; font-size:20px;">KEEP KAGGLING!</p> 
# </span>

# In[96]:


from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

#ä¸­æ–‡å­—ç¬¦æ­£å¸¸æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']

y_pred_prob = eclf1.predict_proba(X_test)[:,1]

print("========== åˆ†ç±»æ€§èƒ½è¯¦æƒ… ==========")
print(classification_report(y_test, predictions, target_names=['æœªæµå¤±', 'æµå¤±']))

print(f"ROC-AUC Score: {roc_auc_score(y_test, y_pred_prob):.4f}")

# ç»˜åˆ¶æ··æ·†çŸ©é˜µ

cm=confusion_matrix(y_test, predictions)
fig, ax = plt.subplots(figsize=(4, 3))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['é¢„æµ‹æœªæµå¤±', 'é¢„æµ‹æµå¤±'],
            yticklabels=['å®é™…æœªæµå¤±', 'å®é™…æµå¤±'])
ax.set_ylabel('å®é™…')
ax.set_xlabel('é¢„æµ‹')
ax.set_title('æ··æ·†çŸ©é˜µ - å®¢æˆ·æµå¤±é¢„æµ‹')
plt.show()

# è®¡ç®—ä¸šåŠ¡å…³é”®æŒ‡æ ‡
tn, fp, fn, tp = cm.ravel()
print(f"========== ä¸šåŠ¡è§†è§’æŒ‡æ ‡ ==========")
print(f"æµå¤±å®¢æˆ·æ•è·ç‡ (å¬å›ç‡ Recall): {tp/(tp+fn):.2%}") # æˆ‘ä»¬æŠ“ä½äº†å¤šå°‘â€œçœŸæµå¤±â€ï¼Ÿ
print(f"é¢„è­¦å‡†ç¡®ç‡ (ç²¾ç¡®ç‡ Precision): {tp/(tp+fp):.2%}") # æˆ‘ä»¬å‘å‡ºçš„æµå¤±é¢„è­¦ä¸­ï¼Œæœ‰å¤šå°‘æ˜¯å¯¹çš„ï¼Ÿ
print(f"è¯¯ä¼¤ç‡ (False Alarm Rate): {fp/(fp+tn):.2%}") # å¤šå°‘å¥½å®¢æˆ·è¢«æˆ‘ä»¬é”™åˆ¤ä¸ºæµå¤±ï¼Ÿ


# In[97]:


# å¯»æ‰¾æœ€ä½³åˆ†ç±»é˜ˆå€¼ï¼Œæ¨¡å‹é»˜è®¤ä»¥0.5ä¸ºç•Œåˆ’åˆ†â€œæµå¤±â€ä¸â€œä¸æµå¤±â€ã€‚æˆ‘ä»¬å¯ä»¥é™ä½è¿™ä¸ªé˜ˆå€¼ï¼Œè®©æ¨¡å‹å˜å¾—æ›´â€œæ•æ„Ÿâ€ã€‚
from sklearn.metrics import precision_recall_curve

# è·å–æµ‹è¯•é›†çš„é¢„æµ‹æ¦‚ç‡ï¼ˆå±äºâ€œæµå¤±â€ç±»çš„æ¦‚ç‡ï¼‰
y_pred_proba = eclf1.predict_proba(X_test)[:, 1]

# è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡
precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)

# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥å¯»æ‰¾æ»¡è¶³ä¸šåŠ¡éœ€æ±‚çš„æœ€ä½³é˜ˆå€¼
def find_best_threshold(target_recall):
    """æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¾¾åˆ°ç›®æ ‡å¬å›ç‡çš„é˜ˆå€¼"""
    for i, recall in enumerate(recalls):
        if recall >= target_recall:
            return thresholds[i], precisions[i], recall
    return thresholds[-1], precisions[-1], recalls[-1]

# ä¸šåŠ¡ç›®æ ‡ï¼šæˆ‘ä»¬å¸Œæœ›è‡³å°‘æŠ“ä½75%çš„æµå¤±å®¢æˆ·ï¼ˆå¬å›ç‡>=0.75ï¼‰
target_recall = 0.75
best_thresh, prec_at_thresh, rec_at_thresh = find_best_threshold(target_recall)

print(f"\n========== é˜ˆå€¼è°ƒä¼˜åˆ†æ ==========")
print(f"å½“è®¾å®šå¬å›ç‡ç›®æ ‡ä¸º {target_recall:.0%} æ—¶ï¼š")
print(f"  æ¨èé˜ˆå€¼: {best_thresh:.3f}")
print(f"  å¯¹åº”ç²¾ç¡®ç‡: {prec_at_thresh:.2%}")
print(f"  å¯¹åº”å¬å›ç‡: {rec_at_thresh:.2%}")

# ä½¿ç”¨æ–°é˜ˆå€¼è¿›è¡Œé¢„æµ‹
y_pred_new = (y_pred_proba >= best_thresh).astype(int)

# é‡æ–°è¯„ä¼°
from sklearn.metrics import classification_report
print(f"\næ–°é˜ˆå€¼ä¸‹çš„åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred_new, target_names=['æœªæµå¤±', 'æµå¤±']))

# ã€é‡è¦ã€‘ä¿å­˜è¿™ä¸ªæœ€ä½³é˜ˆå€¼ï¼Œåœ¨éƒ¨ç½²çš„model_loader.pyä¸­è¦ä½¿ç”¨
print(f"\nè¯·è®°å½•æ­¤é˜ˆå€¼ï¼Œå¹¶æ›´æ–°åˆ° model_loader.py ä¸­: best_threshold = {best_thresh}")


# # XGBoost Classifier
# 

# In[ ]:


from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
  
# ä½¿ç”¨æ›´å…³æ³¨å¬å›ç‡çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆF2-Scoreï¼Œç»™äºˆå¬å›ç‡2å€æƒé‡äºç²¾ç¡®ç‡ï¼‰
xgb = XGBClassifier(random_state=42, eval_metric='logloss')
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1],
    'scale_pos_weight': [1, 2, 3] # æ­¤å‚æ•°ä¸“é—¨å¤„ç†ä¸å¹³è¡¡æ•°æ®ï¼Œå¢å¤§è¯¥å€¼ä¼šæå‡å¯¹â€œæµå¤±â€ï¼ˆæ­£ç±»ï¼‰çš„å…³æ³¨
}
# ä½¿ç”¨å¬å›ç‡ä½œä¸ºè¯„ä¼°æ ‡å‡†
grid_search = GridSearchCV(xgb, param_grid, cv=3, scoring='recall', n_jobs=-1)
grid_search.fit(X_train, y_train)

print(f"XGBoost æœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"XGBoost æœ€ä½³å¬å›ç‡: {grid_search.best_score_:.3f}")

# ç”¨æœ€ä½³æ¨¡å‹é¢„æµ‹
best_xgb = grid_search.best_estimator_
y_pred_xgb = best_xgb.predict(X_test)

print(f"\nXGBoost æµ‹è¯•é›†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred_xgb, target_names=['æœªæµå¤±', 'æµå¤±']))


# ## 1. è°ƒæ•´é˜ˆå€¼
# 

# In[59]:


# å¯»æ‰¾æœ€ä½³åˆ†ç±»é˜ˆå€¼ï¼Œæ¨¡å‹é»˜è®¤ä»¥0.5ä¸ºç•Œåˆ’åˆ†â€œæµå¤±â€ä¸â€œä¸æµå¤±â€ã€‚æˆ‘ä»¬å¯ä»¥é™ä½è¿™ä¸ªé˜ˆå€¼ï¼Œè®©æ¨¡å‹å˜å¾—æ›´â€œæ•æ„Ÿâ€ã€‚
from sklearn.metrics import precision_recall_curve

# è·å–æµ‹è¯•é›†çš„é¢„æµ‹æ¦‚ç‡ï¼ˆå±äºâ€œæµå¤±â€ç±»çš„æ¦‚ç‡ï¼‰

y_pred_proba = best_xgb.predict_proba(X_test)[:, 1] #

# è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡
precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)

# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥å¯»æ‰¾æ»¡è¶³ä¸šåŠ¡éœ€æ±‚çš„æœ€ä½³é˜ˆå€¼
def find_best_threshold(target_recall):
    """æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¾¾åˆ°ç›®æ ‡å¬å›ç‡çš„é˜ˆå€¼"""
    for i, recall in enumerate(recalls):
        if recall >= target_recall:
            return thresholds[i], precisions[i], recall
    return thresholds[-1], precisions[-1], recalls[-1]

# ä¸šåŠ¡ç›®æ ‡ï¼šæˆ‘ä»¬å¸Œæœ›è‡³å°‘æŠ“ä½75%çš„æµå¤±å®¢æˆ·ï¼ˆå¬å›ç‡>=0.75ï¼‰
# target_recall = 0.75
# best_thresh, prec_at_thresh, rec_at_thresh = find_best_threshold(target_recall)

print(f"\n========== é˜ˆå€¼è°ƒä¼˜åˆ†æ ==========")
print(f"å½“è®¾å®šå¬å›ç‡ç›®æ ‡ä¸º {target_recall:.0%} æ—¶ï¼š")
print(f"  æ¨èé˜ˆå€¼: {best_thresh:.3f}")
print(f"  å¯¹åº”ç²¾ç¡®ç‡: {prec_at_thresh:.2%}")
print(f"  å¯¹åº”å¬å›ç‡: {rec_at_thresh:.2%}")

new_thresh = 0.42
# ä½¿ç”¨æ–°é˜ˆå€¼è¿›è¡Œé¢„æµ‹
y_pred_new = (y_pred_proba >= new_thresh).astype(int)

# é‡æ–°è¯„ä¼°
from sklearn.metrics import classification_report
print(f"\næ–°é˜ˆå€¼ä¸‹çš„xgbåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred_new, target_names=['æœªæµå¤±', 'æµå¤±']))

# ã€é‡è¦ã€‘ä¿å­˜è¿™ä¸ªæœ€ä½³é˜ˆå€¼ï¼Œåœ¨éƒ¨ç½²çš„model_loader.pyä¸­è¦ä½¿ç”¨
print(f"\nè¯·è®°å½•æ­¤é˜ˆå€¼ï¼Œå¹¶æ›´æ–°åˆ° model_loader.py ä¸­: best_threshold = {best_thresh}")


# ## 2. ç‰¹å¾é‡è¦æ€§åˆ†æ
# 

# In[53]:


import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['SimHei']

# æå–XGBoostæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
xgb_feature_importance = best_xgb.feature_importances_

# # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
# plt.figure(figsize=(10, 6))
# plt.barh(X_train.columns, xgb_feature_importance)
# plt.title('XGBoost ç‰¹å¾é‡è¦æ€§')
# plt.xlabel('ç‰¹å¾é‡è¦æ€§åˆ†æ•°')
# plt.ylabel('ç‰¹å¾')
# plt.show()

features = X_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': xgb_feature_importance})
importance_df = importance_df.sort_values('importance', ascending=False)

print("========== ç‰¹å¾é‡è¦æ€§ Top 10 ==========")
print(importance_df.head(10))

# å¯è§†åŒ–
import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
plt.barh(importance_df.head(10)['feature'], importance_df.head(10)['importance'])
plt.xlabel('Importance')
plt.title('Top 10 Feature Importance (GradientBoosting)')
plt.gca().invert_yaxis()
plt.show()


# In[ ]:


# import pickle
# import joblib
# from tensorflow.keras.models import save_model

# ç¤ºä¾‹1ï¼š
# joblib.dump(eclf1, 'random_forest_model.joblib')  # ä¿å­˜ä¸ºjoblibæ–‡ä»¶
# print("ok")
# # ç¤ºä¾‹2ï¼šTensorFlow/Kerasæ¨¡å‹
# model = Sequential([...])
# model.fit(X_train, y_train)
# model.save('neural_network_model.h5')  # ä¿å­˜ä¸ºH5æ–‡ä»¶
# 'VotingClassifier' object has no attribute 'save',eclf1è½¯æŠ•ç¥¨æ¨¡å‹ä¸èƒ½ç”¨è¿™ä¸ª.save

# ç¤ºä¾‹3ï¼šé€šç”¨pickleæ–¹æ³•
# pickle.dump(eclf1, open('model.pkl', 'wb'))

# import os
# æŸ¥çœ‹å½“å‰å·¥ä½œç›®å½•
# print(os.getcwd())  /kaggle/working
# ä¸Šä¼ åˆ°Kaggleæ•°æ®é›†ï¼ˆéœ€åœ¨Notebookä¸­æ‰§è¡Œï¼‰
# !mkdir my_model_dataset
# æŸ¥çœ‹åˆ›å»ºçš„æ–‡ä»¶å¤¹è·¯å¾„
# folder_path = os.path.join(os.getcwd(), 'my_model_dataset')
# print(folder_path)  # è¾“å‡ºï¼š/kaggle/working/my_model_dataset

# # ç¡®è®¤æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
# print(os.path.exists(folder_path))  # è¾“å‡ºï¼šTrue

# !mv *.joblib my_model_dataset/
# !mv *.h5 my_model_dataset/


# In[83]:


import joblib

# ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶
model_path = 'voting_classifier_model.joblib'
joblib.dump(eclf1, model_path)
print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

# åœ¨éƒ¨ç½²ç¯å¢ƒä¸­åŠ è½½æ¨¡å‹
loaded_model = joblib.load(model_path)
print("æ¨¡å‹åŠ è½½æˆåŠŸ")

# ä½¿ç”¨åŠ è½½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
new_predictions = loaded_model.predict(X_test)
print(f"é¢„æµ‹ç»“æœç¤ºä¾‹: {new_predictions[:5]}")  


# In[60]:


import joblib
model_path = 'demo/models/optimized_xgb_churn_model.pkl'  # ä½¿ç”¨æ–°åå­—
joblib.dump(best_xgb, model_path)  # best_xgb æ˜¯ä½ çš„ GridSearchCV æœ€ä½³æ¨¡å‹
print(f"ä¼˜åŒ–åçš„XGBoostæ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")


# In[84]:


X_test


# In[85]:


import pandas as pd
import numpy as np
import sklearn
print(f"pandas version: {pd.__version__}")
print(f"numpy version: {np.__version__}")
print(f"scikit-learn version: {sklearn.__version__}")

