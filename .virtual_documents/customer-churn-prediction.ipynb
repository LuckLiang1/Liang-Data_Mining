














import pandas as pd
import numpy as np
#missingno模块（缺失值可视化）
import missingno as msno  
# Matplotlib 是 Python 的绘图库，可以用来绘制各种静态，动态，交互式的图表，提供多样化的输出格式。
# 通常与 NumPy 和 SciPy（Scientific Python）一起使用， 这种组合广泛用于替代 MatLab
# SciPy 包含的模块有最优化、线性代数、积分、插值、特殊函数、快速傅里叶变换、信号处理和图像处理、常微分方程求解和其他科学与工程中常用的计算
import matplotlib.pyplot as plt
# Seaborn integrates closely with Pandas data structures, 
# making it easy to work with dataframes and arrays
# 建立在 Matplotlib 基础之上的 Python 数据可视化库，专注于绘制各种统计图形
import seaborn as sns
# Plotly Express 是一个高级的Python数据可视化库，它是Plotly.py的封装，提供了一个简洁且一致的API来创建复杂的图表。
import plotly.express as px
# Plotly 是一个强大的 Python 数据可视化库，提供了丰富的图表类型和灵活的定制选项。Plotly 的图形对象（Graph Objects）模块（通常导入为 go）包含了一系列自动生成的 Python 类，这些类表示图形的各个部分
import plotly.graph_objects as go
# 绘制子图
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')


# StandardScaler是sklearn中的一个归一化工具，可以对每个特征维度进行去均值和方差标准化，使数据符合标准正态分布
from sklearn.preprocessing import StandardScaler
# LabelEncoder 是 sklearn 中用于类别标签编码的重要工具，能够将离散的类别型标签转换为模型可识别的数值格式
from sklearn.preprocessing import LabelEncoder
# 决策树分类器
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
# 高斯朴素贝叶斯 先验概率priors
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
# MLPClassifier 是一个监督学习算法，它是多层感知机（MLP）的一种，也称为人工神经网络（ANN）。MLPClassifier可以处理包括分类问题在内的多种机器学习任务。它通过学习输入和输出之间的映射关系来进行预测。
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
# 极端随机树
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# XGBClassifier是基于梯度提升的机器学习算法，它可以处理缺失值、支持并行计算，并且具有内置的交叉验证功能
from xgboost import XGBClassifier
# CatBoost是一个高性能的机器学习库，它基于对称决策树（oblivious trees）作为基学习器，能够有效处理类别型特征。CatBoostClassifier是CatBoost库中用于分类问题的组件，它提供了丰富的参数用于模型的训练和优化
from catboost import CatBoostClassifier
from sklearn import metrics
from sklearn.metrics import roc_curve
from sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report


#loading data
# df = pd.read_csv('../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')
df=pd.read_csv(r'E:\Code\Python\customer-churn-prediction\data\raw\WA_Fn-UseC_-Telco-Customer-Churn.csv') 












df.head()





df.shape


df.info()


df.columns.values


df.dtypes











# Visualize missing values as a matrix
msno.matrix(df);











df = df.drop(['customerID'], axis = 1)
df.head()





# pandas.to_numeric() 是 pandas 顶级函数，语法是
# pandas.to_numeric(arg, errors='raise', downcast=None)
# errors : 可传入 {'ignore', 'raise', 'coerce'}, 默认 'raise'，如果无法解析数据的处理方案。
# 'raise', 如果无法解析将引发异常
# 'coerce', 如果无法解析将设置为 NaN
# 'ignore', 然后无效解析将返回输入
# downcast : str, 默认 None，降级处理、向下转换。可传入值有 'integer', 'signed', 'unsigned', 或者 'float'。
df['TotalCharges'] = pd.to_numeric(df.TotalCharges, errors='coerce')
df.isnull().sum()





df[np.isnan(df['TotalCharges'])]





df[df['tenure'] == 0].index





df.drop(labels=df[df['tenure'] == 0].index, axis=0, inplace=True)
df[df['tenure'] == 0].index
df1=df.copy()
df1.info()
df1['TotalCharges'] = pd.to_numeric(df1.TotalCharges, errors='coerce')
df1.isnull().sum()





# 原本的df已经删除缺失的11行，并被替换了，为啥还插值？
df.fillna(df["TotalCharges"].mean())


df.isnull().sum()


df["SeniorCitizen"]= df["SeniorCitizen"].map({0: "No", 1: "Yes"})
df.head()


df["InternetService"].describe(include=['object', 'bool'])


numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']
df[numerical_cols].describe()








g_labels = ['Male', 'Female']
c_labels = ['No', 'Yes']
# Create subplots: use 'domain' type for Pie subplot
fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])
fig.add_trace(go.Pie(labels=g_labels, values=df['gender'].value_counts(), name="Gender"),
              1, 1)
fig.add_trace(go.Pie(labels=c_labels, values=df['Churn'].value_counts(), name="Churn"),
              1, 2)

# Use `hole` to create a donut-like pie chart
fig.update_traces(hole=.4, hoverinfo="label+percent+name", textfont_size=16)

fig.update_layout(
    title_text="Gender and Churn Distributions",
    # Add annotations in the center of the donut pies.
    annotations=[dict(text='Gender', x=0.16, y=0.5, font_size=20, showarrow=False),
                 dict(text='Churn', x=0.84, y=0.5, font_size=20, showarrow=False)])
fig.show()





df["Churn"][df["Churn"]=="No"].groupby(by=df["gender"]).count()
# df["Churn"][df["Churn"]=="No"].groupby(by=df["gender"]).count()


df["Churn"][df["Churn"]=="Yes"].groupby(by=df["gender"]).count()


plt.figure(figsize=(6, 6))
labels =["Churn: Yes","Churn:No"]
values = [1869,5163]
labels_gender = ["F","M","F","M"]
sizes_gender = [939,930 , 2544,2619]
colors = ['#ff6666', '#66b3ff']
colors_gender = ['#c2c2f0','#ffb3e6', '#c2c2f0','#ffb3e6']
explode = (0.3,0.3) 
explode_gender = (0.1,0.1,0.1,0.1)
textprops = {"fontsize":15}
#Plot
plt.pie(values, labels=labels,autopct='%1.1f%%',pctdistance=1.08, labeldistance=0.8,colors=colors, startangle=90,frame=True, explode=explode,radius=10, textprops =textprops, counterclock = True, )
plt.pie(sizes_gender,labels=labels_gender,colors=colors_gender,startangle=90, explode=explode_gender,radius=7, textprops =textprops, counterclock = True, )
#Draw circle
centre_circle = plt.Circle((0,0),5,color='black', fc='white',linewidth=0)
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

plt.title('Churn Distribution w.r.t Gender: Male(M), Female(F)', fontsize=15, y=1.1)

# show plot 
 
plt.axis('equal')
plt.tight_layout()
plt.show()





fig = px.histogram(df, x="Churn", color="Contract", barmode="group", title="<b>Customer contract distribution<b>")
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()





labels = df['PaymentMethod'].unique()
values = df['PaymentMethod'].value_counts()
print(labels)
print(values)

fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])
fig.update_layout(title_text="<b>Payment Method Distribution</b>")
fig.show()


fig = px.histogram(df, x="Churn", color="PaymentMethod", title="<b>Customer Payment Method distribution w.r.t. Churn</b>")
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()





df["InternetService"].unique()


df[df["gender"]=="Male"][["InternetService", "Churn"]].value_counts()


df[df["gender"]=="Female"][["InternetService", "Churn"]].value_counts()


fig = go.Figure()

fig.add_trace(go.Bar(
  x = [['Churn:No', 'Churn:No', 'Churn:Yes', 'Churn:Yes'],
       ["Female", "Male", "Female", "Male"]],
  y = [965, 992, 219, 240],
  name = 'DSL',
))

fig.add_trace(go.Bar(
  x = [['Churn:No', 'Churn:No', 'Churn:Yes', 'Churn:Yes'],
       ["Female", "Male", "Female", "Male"]],
  y = [889, 910, 664, 633],
  name = 'Fiber optic',
))

fig.add_trace(go.Bar(
  x = [['Churn:No', 'Churn:No', 'Churn:Yes', 'Churn:Yes'],
       ["Female", "Male", "Female", "Male"]],
  y = [690, 717, 56, 57],
  name = 'No Internet',
))

fig.update_layout(title_text="<b>Churn Distribution w.r.t. Internet Service and Gender</b>")

fig.show()





color_map = {"Yes": "#FF97FF", "No": "#AB63FA"}
fig = px.histogram(df, x="Churn", color="Dependents", barmode="group", title="<b>Dependents distribution</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()





color_map = {"Yes": '#FFA15A', "No": '#00CC96'}
fig = px.histogram(df, x="Churn", color="Partner", barmode="group", title="<b>Chrun distribution w.r.t. Partners</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()





color_map = {"Yes": '#00CC96', "No": '#B6E880'}
fig = px.histogram(df, x="Churn", color="SeniorCitizen", title="<b>Chrun distribution w.r.t. Senior Citizen</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()





color_map = {"Yes": "#FF97FF", "No": "#AB63FA"}
fig = px.histogram(df, x="Churn", color="OnlineSecurity", barmode="group", title="<b>Churn w.r.t Online Security</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()





color_map = {"Yes": '#FFA15A', "No": '#00CC96'}
fig = px.histogram(df, x="Churn", color="PaperlessBilling",  title="<b>Chrun distribution w.r.t. Paperless Billing</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()





fig = px.histogram(df, x="Churn", color="TechSupport",barmode="group",  title="<b>Chrun distribution w.r.t. TechSupport</b>")
fig.update_layout(width=700, height=500, bargap=0.1)
try:
    fig.show()
except Exception as e:
    print(f"Error displaying figure: {e}")   





color_map = {"Yes": '#00CC96', "No": '#B6E880'}
fig = px.histogram(df, x="Churn", color="PhoneService", title="<b>Chrun distribution w.r.t. Phone Service</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()





# 核密度估计（KDE）是一种用于估计随机变量概率密度函数的非参数方法。在seaborn库中，kdeplot函数提供了一种方便的方式来可视化单变量或双变量的分布。
# 这个函数会生成一个连续的概率密度曲线，可以帮助我们理解数据的分布特征。
# 数据分布（Data Distribution） 指的是一组数据中各个值的出现频率或概率，描述了数据在数轴上的分布形态、集中趋势、离散程度等特征。
sns.set_context("paper",font_scale=1.1)
ax = sns.kdeplot(df.MonthlyCharges[(df["Churn"] == 'No') ],
                color="Red", shade = True);
ax = sns.kdeplot(df.MonthlyCharges[(df["Churn"] == 'Yes') ],
                ax =ax, color="Blue", shade= True);
ax.legend(["Not Churn","Churn"],loc='upper right');
ax.set_ylabel('Density');
ax.set_xlabel('Monthly Charges');
ax.set_title('Distribution of monthly charges by churn');






ax = sns.kdeplot(df.TotalCharges[(df["Churn"] == 'No') ],
                color="Gold", shade = True);
ax = sns.kdeplot(df.TotalCharges[(df["Churn"] == 'Yes') ],
                ax =ax, color="Green", shade= True);
ax.legend(["Not Chu0rn","Churn"],loc='upper right');
ax.set_ylabel('Density');
ax.set_xlabel('Total Charges');
ax.set_title('Distribution of total charges by churn');


fig = px.box(df, x='Churn', y = 'tenure')

# Update yaxis properties
fig.update_yaxes(title_text='Tenure (Months)', row=1, col=1)
# Update xaxis properties
fig.update_xaxes(title_text='Churn', row=1, col=1)

# Update size and title
fig.update_layout(autosize=True, width=750, height=600,
    title_font=dict(size=25, family='Courier'),
    title='<b>Tenure vs Churn</b>',
)

fig.show()






plt.figure(figsize=(25, 10))

# df.apply(function,axis) 遍历一行axis=1或一列axis=0(默认)
# lambda:函数式编程
# # factorize()  Example array 分类变量转换为整数编码
# arr = np.array(['b', 'b', 'a', 'c', 'b'], dtype="O")
# # Factorize the array
# codes, uniques = pd.factorize(arr)
# print("Codes:", codes) # Output: [0, 0, 1, 2, 0]
# print("Uniques:", uniques) # Output: ['b', 'a', 'c']
# corr通常是一个相关系数矩阵（如通过pandas.DataFrame.corr()计算得到），形状为(n, n)
corr = df.apply(lambda x: pd.factorize(x)[0]).corr()   

# np.ones_like()创建一个与给定数组形状和类型相同的新数组，但新数组的所有元素都是1
# np.triu 是NumPy库中的一个函数，用于提取矩阵的上三角?部分
mask = np.triu(np.ones_like(corr, dtype=bool))
# 使用seaborn.heatmap()传入掩码，避免重复显示对称信息
ax = sns.heatmap(
    corr,                   # 相关系数矩阵（通常是DataFrame或numpy数组）
    mask=mask,              # 掩码：隐藏下三角部分（包括对角线）
    xticklabels=corr.columns,  # X轴标签使用DataFrame的列名
    yticklabels=corr.columns,  # Y轴标签使用DataFrame的列名
    annot=True,             # 在每个单元格中显示相关系数值
    linewidths=.2,          # 单元格之间的分隔线宽度
    cmap='coolwarm',        # 颜色映射：从蓝色（-1）到白色（0）到红色（+1）
    vmin=-1, vmax=1         # 颜色映射的取值范围：相关系数的理论范围是[-1, 1]
)











def object_to_int(dataframe_series):
    if dataframe_series.dtype=='object':
        dataframe_series = LabelEncoder().fit_transform(dataframe_series) # 将字符串编码为整数
    return dataframe_series


# df.apply(...)：对 DataFrame 的每一列（默认 axis=0）应用自定义函数。
df = df.apply(lambda x: object_to_int(x))  #匿名函数，其中 x 代表 DataFrame 的每一列（即一个 pandas Series）
df.head()


plt.figure(figsize=(14,7))
# print(df.corr())
df.corr()
# 计算 DataFrame 中所有数值列之间的皮尔逊相关系数（默认方法），返回一个相关系数矩阵
# ['Churn']：从相关系数矩阵中提取与Churn列相关的所有系数，得到一个 Series
df.corr()['Churn'].sort_values(ascending = False)


X = df.drop(columns = ['Churn'])
y = df['Churn'].values
print(df["Churn"])
print(df['Churn'].values)


# stratify=y
# 分层抽样：确保训练集和测试集中目标变量 y 的类别比例与原始数据一致。
# 适用场景：处理不平衡数据集（如正类样本占 10%，负类占 90%），防止训练 / 测试集分布偏差导致模型失效。
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30, random_state = 40, stratify=y)

# 确保在划分后再进行特征工程（如标准化、特征选择）。若在划分前处理，测试集可能 “偷看” 到训练集的统计信息。


def distplot(feature, frame, color='r'):
    plt.figure(figsize=(8,3))
    plt.title("Distribution for {}".format(feature))
    ax = sns.distplot(frame[feature], color= color)





num_cols = ["tenure", 'MonthlyCharges', 'TotalCharges']
for feat in num_cols: distplot(feat, df)








df_std = pd.DataFrame(StandardScaler().fit_transform(df[num_cols].astype('float64')),
                       columns=num_cols)
for feat in numerical_cols: distplot(feat, df_std, color='c')





# Divide the columns into 3 categories, one ofor standardisation, one for label encoding and one for one hot encoding
# 手动指定需要独热编码的列（通常是无序分类变量）:
cat_cols_ohe =['PaymentMethod', 'Contract', 'InternetService'] # those that need one-hot encoding
cat_cols_le = list(set(X_train.columns)- set(num_cols) - set(cat_cols_ohe)) #those that need label encoding
print("需要标签编码的列:",cat_cols_le)


scaler= StandardScaler()

X_train[num_cols] = scaler.fit_transform(X_train[num_cols])
X_test[num_cols] = scaler.transform(X_test[num_cols])











knn_model = KNeighborsClassifier(n_neighbors = 11, n_jobs=1) 
knn_model.fit(X_train,y_train)
predicted_y = knn_model.predict(X_test)
accuracy_knn = knn_model.score(X_test,y_test)
print("KNN accuracy:",accuracy_knn)


print(classification_report(y_test, predicted_y))





svc_model = SVC(random_state = 1)
svc_model.fit(X_train,y_train)
predict_y = svc_model.predict(X_test)
accuracy_svc = svc_model.score(X_test,y_test)
print("SVM accuracy is :",accuracy_svc)


print(classification_report(y_test, predict_y))





model_rf = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,
                                  random_state =50, max_features = "sqrt",
                                  max_leaf_nodes = 30)
model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print (metrics.accuracy_score(y_test, prediction_test))
accuracy_rf = model_rf.score(X_test,y_test)
print("RF accuracy is :",accuracy_rf)


print(classification_report(y_test, prediction_test))


plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, prediction_test),
                annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title(" RANDOM FOREST CONFUSION MATRIX",fontsize=14)
plt.show()


y_rfpred_prob = model_rf.predict_proba(X_test)[:,1]
fpr_rf, tpr_rf, thresholds = roc_curve(y_test, y_rfpred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr_rf, tpr_rf, label='Random Forest',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest ROC Curve',fontsize=16)
plt.show();





lr_model = LogisticRegression()
lr_model.fit(X_train,y_train)
accuracy_lr = lr_model.score(X_test,y_test)
print("Logistic Regression accuracy is :",accuracy_lr)


lr_pred= lr_model.predict(X_test)
report = classification_report(y_test,lr_pred)
print(report)


plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, lr_pred),
                annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("LOGISTIC REGRESSION CONFUSION MATRIX",fontsize=14)
plt.show()


y_pred_prob = lr_model.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='Logistic Regression',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression ROC Curve',fontsize=16)
plt.show();





dt_model = DecisionTreeClassifier()
dt_model.fit(X_train,y_train)
predictdt_y = dt_model.predict(X_test)
accuracy_dt = dt_model.score(X_test,y_test)
print("Decision Tree accuracy is :",accuracy_dt)








print(classification_report(y_test, predictdt_y))





a_model = AdaBoostClassifier()
a_model.fit(X_train,y_train)
a_preds = a_model.predict(X_test)
print("AdaBoost Classifier accuracy")
metrics.accuracy_score(y_test, a_preds)


print(classification_report(y_test, a_preds))


plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, a_preds),
                annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("AdaBoost Classifier Confusion Matrix",fontsize=14)
plt.show()





gb = GradientBoostingClassifier()
gb.fit(X_train, y_train)
gb_pred = gb.predict(X_test)
print("Gradient Boosting Classifier", accuracy_score(y_test, gb_pred))


print(classification_report(y_test, gb_pred))


plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, gb_pred),
                annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("Gradient Boosting Classifier Confusion Matrix",fontsize=14)
plt.show()





from sklearn.ensemble import VotingClassifier
# GradientBoostingClassifier：梯度提升树，适合捕捉数据中的非线性关系
# LogisticRegression：逻辑回归，提供线性分类边界和概率输出
# AdaBoostClassifier：自适应提升算法，通过组合弱分类器提高整体性能
clf1 = GradientBoostingClassifier()
clf2 = LogisticRegression()
clf3 = AdaBoostClassifier()
# estimators 参数是一个元组列表，每个元组包含 (名称，模型)
# voting='soft' 表示使用软投票机制：基于各模型的预测概率进行加权平均
# 软投票要求所有基础模型都能提供概率预测（即具有 predict_proba 方法）
eclf1 = VotingClassifier(estimators=[('gbc', clf1), ('lr', clf2), ('abc', clf3)], voting='soft')
eclf1.fit(X_train, y_train)
predictions = eclf1.predict(X_test)
print("Final Accuracy Score ")
print(accuracy_score(y_test, predictions))


print(classification_report(y_test, predictions))


plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, predictions),
                annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("FINAL CONFUSION MATRIX",fontsize=14)
plt.show()











import pickle
import joblib
# from tensorflow.keras.models import save_model

# 示例1：
joblib.dump(eclf1, 'random_forest_model.joblib')  # 保存为joblib文件
print("ok")
# # 示例2：TensorFlow/Keras模型
# model = Sequential([...])
# model.fit(X_train, y_train)
# model.save('neural_network_model.h5')  # 保存为H5文件
# 'VotingClassifier' object has no attribute 'save',eclf1软投票模型不能用这个.save

# 示例3：通用pickle方法
pickle.dump(eclf1, open('model.pkl', 'wb'))

# import os
# 查看当前工作目录
# print(os.getcwd())  /kaggle/working
# 上传到Kaggle数据集（需在Notebook中执行）
# !mkdir my_model_dataset
# 查看创建的文件夹路径
# folder_path = os.path.join(os.getcwd(), 'my_model_dataset')
# print(folder_path)  # 输出：/kaggle/working/my_model_dataset

# # 确认文件夹是否存在
# print(os.path.exists(folder_path))  # 输出：True

# !mv *.joblib my_model_dataset/
# !mv *.h5 my_model_dataset/


import joblib

# 保存模型到文件
model_path = 'voting_classifier_model.joblib'
joblib.dump(eclf1, model_path)
print(f"模型已保存到: {model_path}")

# 在部署环境中加载模型
loaded_model = joblib.load(model_path)
print("模型加载成功")

# 使用加载的模型进行预测
new_predictions = loaded_model.predict(X_test)
print(f"预测结果示例: {new_predictions[:5]}")  


X_test


import pandas as pd
import numpy as np
import sklearn
print(f"pandas version: {pd.__version__}")
print(f"numpy version: {np.__version__}")
print(f"scikit-learn version: {sklearn.__version__}")
